# Zen MCP Server Model Metadata
# This file contains model-specific metadata that LiteLLM doesn't natively support
# Used by LiteLLMProvider for capability detection and special handling

models:
  # OpenAI Models
  o3:
    friendly_name: "OpenAI O3"
    context_window: 200000
    max_output_tokens: 100000
    supports_extended_thinking: false
    supports_temperature: false  # Fixed at 1.0
    temperature_constraint: "fixed"
    supports_images: false
    supports_json_mode: true
    description: "OpenAI's O3 model with advanced reasoning"
    
  o3-mini:
    friendly_name: "OpenAI O3 Mini"
    context_window: 200000
    max_output_tokens: 65536
    supports_extended_thinking: false
    supports_temperature: false  # Fixed at 1.0
    temperature_constraint: "fixed"
    supports_images: false
    supports_json_mode: true
    description: "Smaller, faster O3 variant"
    
  o3-pro:
    friendly_name: "OpenAI O3 Pro"
    context_window: 200000
    max_output_tokens: 100000
    supports_extended_thinking: false
    supports_temperature: false  # Fixed at 1.0
    temperature_constraint: "fixed"
    supports_images: false
    supports_json_mode: true
    description: "Professional-grade O3 model (expensive)"
    
  o3-deep-research:
    friendly_name: "OpenAI O3 Deep Research"
    context_window: 200000
    max_output_tokens: 100000
    supports_extended_thinking: false
    supports_temperature: false  # Fixed at 1.0
    temperature_constraint: "fixed"
    supports_images: false
    supports_json_mode: true
    description: "Deep research model for comprehensive analysis"
    
  o4-mini:
    friendly_name: "OpenAI O4 Mini"
    context_window: 200000
    max_output_tokens: 16384
    supports_extended_thinking: false
    supports_temperature: false  # Fixed at 1.0
    temperature_constraint: "fixed"
    supports_images: true
    max_image_size_mb: 20.0
    supports_json_mode: true
    description: "Latest reasoning model optimized for shorter contexts"
    
  gpt-4.1:
    friendly_name: "GPT-4.1"
    context_window: 1048576  # 1M tokens
    max_output_tokens: 131072
    supports_extended_thinking: false
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: true
    max_image_size_mb: 20.0
    supports_json_mode: true
    description: "Advanced reasoning model with large context window"

  # Google Gemini Models
  gemini-2.5-flash:
    friendly_name: "Gemini Flash"
    context_window: 1048576  # 1M tokens
    max_output_tokens: 8192
    supports_extended_thinking: true
    max_thinking_tokens: 24576
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: true
    max_image_size_mb: 20.0
    supports_json_mode: true
    description: "Ultra-fast model with thinking support"
    
  gemini-2.5-pro:
    friendly_name: "Gemini Pro"
    context_window: 1048576  # 1M tokens
    max_output_tokens: 8192
    supports_extended_thinking: true
    max_thinking_tokens: 32768
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: true
    max_image_size_mb: 32.0  # Pro models support larger images
    supports_json_mode: true
    description: "Deep reasoning model with extended thinking"
    
  gemini-2.0-flash:
    friendly_name: "Gemini 2.0 Flash"
    context_window: 1048576  # 1M tokens
    max_output_tokens: 1048576  # 1M tokens
    supports_extended_thinking: true
    max_thinking_tokens: 24576
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: true
    max_image_size_mb: 20.0
    supports_json_mode: false  # 2.0 models don't support JSON mode yet
    description: "Latest fast model with experimental thinking"
    
  gemini-2.0-flash-lite:
    friendly_name: "Gemini 2.0 Flash Lite"
    context_window: 1048576  # 1M tokens
    max_output_tokens: 1048576  # 1M tokens
    supports_extended_thinking: false  # Lite model doesn't support thinking
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: false  # Text-only model
    supports_json_mode: false
    description: "Lightweight fast model, text-only"

  # X.AI GROK Models
  grok-4:
    friendly_name: "GROK-4"
    context_window: 131072
    max_output_tokens: 131072
    supports_extended_thinking: true  # GROK-4 supports extended reasoning
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: false
    supports_json_mode: false
    description: "Latest advanced reasoning model from X.AI"
    
  grok-3:
    friendly_name: "GROK-3"
    context_window: 131072
    max_output_tokens: 131072
    supports_extended_thinking: false
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: false
    supports_json_mode: false
    description: "Previous generation reasoning model from X.AI"
    
  grok-3-fast:
    friendly_name: "GROK-3 Fast"
    context_window: 131072
    max_output_tokens: 131072
    supports_extended_thinking: false
    supports_temperature: true
    temperature_constraint: "range"
    supports_images: false
    supports_json_mode: false
    description: "Higher performance variant, faster but more expensive"

# Model selection priorities (for auto mode)
model_priorities:
  default:
    - gemini-2.5-flash
    - gpt-4.1
    - grok-3-fast
  
  thinking:
    - gemini-2.5-pro
    - gemini-2.5-flash
    - grok-4
  
  images:
    - gemini-2.5-flash
    - gemini-2.5-pro
    - gpt-4.1
    - o4-mini
  
  large_context:
    - gpt-4.1
    - gemini-2.0-flash
    - gemini-2.5-flash
  
  fast:
    - gemini-2.5-flash
    - grok-3-fast
    - gemini-2.0-flash-lite